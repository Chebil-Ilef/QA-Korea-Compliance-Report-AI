APPROACH EXPLANATION: 

1. PDF Text Extraction:
   - I used PyMuPDF (fitz) to convert the PDF document into raw text.
   
2. Question-Answering Pipeline:
   - CSV Reading:  read questions from a CSV file and then iterated over them to find answers from the extracted PDF text.
   
---(2 solutions)

***Solution 1: Hugging Face's `deepset/roberta-base-squad2`
- Model Choice: `deepset/roberta-base-squad2` is a well-known fine-tuned model for the **SQuAD** question-answering dataset. Itâ€™s optimized for shorter, more direct contexts and performs well when questions are precise and the context is manageable (e.g., within a single paragraph or a few sentences).
  
- Process:
  - For each question, the context from the converted PDF was passed to the model.
  - The answers were then written to a CSV file for later analysis.

----> Observations:
  - Correct answers for numerical questions**: This suggests that the model handles structured, direct answers (like numbers, dates, or facts) well.
  - Inaccuracy for others: The model struggles when the context is long or complex, which is common for models trained on smaller contexts (like **SQuAD**, which focuses on short answers from shorter paragraphs).
  
- Limitations:
  - Context Length: The `roberta-base-squad2` model has a limited input size (around 512 tokens), so it cannot effectively process long contexts or large documents without truncating parts of the text.
  - Generalization: While it works well for direct factual answers, it struggles with more nuanced, inferential questions.

***Solution 2: Using `meta-llama/Llama-3.2-1B`
- Model Choice: The Llama-3.2-1B model is a large transformer with 1.24 billion parameters. It's better suited for handling longer context sizes due to its architecture and training on a more diverse range of data.
  
- Observations:
  - Better Results: Since Llama-3.2-1B can process larger contexts, it generated +better results for more complex questions, showing an improvement over the previous Hugging Face model.
  - Context Handling: It performs better with extended context lengths, allowing it to generate more accurate and nuanced answers for questions that require understanding across multiple paragraphs or sections of the document.

- Advantages:
  - Larger Contexts: Llama's architecture allows it to take in more text at once, making it capable of handling large, unstructured PDFs without truncating.

NOTE: cleaned the model response by hand in csv because I didn't have enough time.

***Other Potential Solutions to Explore:

---using openai api yet I needed to request it first
OR
---converting pdf to a faiss vector database then using langchain and CTransformers to work with embeddings

